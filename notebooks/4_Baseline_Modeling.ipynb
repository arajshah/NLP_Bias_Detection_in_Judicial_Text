{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUB-PHASE 4.1: DATA LOAD & TARGET PREP ===\n",
      "Original target dataset loaded: (76314, 19) shape\n",
      "Features dataset loaded: (76314, 433) shape\n",
      "Final merged dataset: (76314, 436) shape\n",
      "\n",
      "Quick checks on merged data:\n",
      "        AGE  CRIMHIST    SENTYR  NUMDEPEN  NEWRACE_Asian or Pacific Islander  \\\n",
      "0 -0.218985  0.400021 -1.803579  0.263778                              False   \n",
      "1 -0.036088  0.400021 -1.803579  0.263778                              False   \n",
      "2  1.244187  0.400021 -1.803579 -0.330798                              False   \n",
      "\n",
      "   NEWRACE_Black  NEWRACE_Hispanic  NEWRACE_Other  NEWRACE_Unknown  \\\n",
      "0          False              True          False            False   \n",
      "1          False              True          False            False   \n",
      "2          False              True          False            False   \n",
      "\n",
      "   NEWRACE_White  ...  SENTMON_July  SENTMON_June  SENTMON_March  SENTMON_May  \\\n",
      "0          False  ...         False         False          False        False   \n",
      "1          False  ...         False         False          False        False   \n",
      "2          False  ...         False         False          False        False   \n",
      "\n",
      "   SENTMON_November  SENTMON_October  SENTMON_September     DISPOSIT  \\\n",
      "0             False             True              False  Guilty plea   \n",
      "1             False             True              False  Guilty plea   \n",
      "2             False             True              False  Guilty plea   \n",
      "\n",
      "    SENTTOT  SENTTOT_RAW  \n",
      "0 -0.327479        30.00  \n",
      "1 -0.378971         2.03  \n",
      "2 -0.349571        18.00  \n",
      "\n",
      "[3 rows x 436 columns]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# PHASE 4: BASELINE MODELING\n",
    "################################################################################\n",
    "\n",
    "###############################\n",
    "#        IMPORTS & SETUP\n",
    "###############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Preprocessing & Splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models for Classification & Regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "###############################\n",
    "#   PATHS / DATA LOAD\n",
    "###############################\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(\"=== SUB-PHASE 4.1: DATA LOAD & TARGET PREP ===\")\n",
    "\n",
    "feature_file = DATA_DIR / \"cleaned_data_phase3.csv\"\n",
    "original_file = DATA_DIR / \"cleaned_data_phase3_unencoded_DISPOSIT.csv\"\n",
    "\n",
    "# Load original (targets + unencoded columns)\n",
    "original_df = pd.read_csv(original_file, low_memory=False)\n",
    "print(f\"Original target dataset loaded: {original_df.shape} shape\")\n",
    "\n",
    "# Load encoded features if available; otherwise build them here as a fallback\n",
    "if feature_file.exists():\n",
    "    encoded_features_df = pd.read_csv(feature_file, low_memory=False)\n",
    "    print(f\"Features dataset loaded: {encoded_features_df.shape} shape\")\n",
    "else:\n",
    "    print(f\"WARNING: missing {feature_file}. Building one-hot encoded features as fallback...\")\n",
    "\n",
    "    # Keep a feature-only frame (drop targets)\n",
    "    feature_df = original_df.copy()\n",
    "    for c in [\"DISPOSIT\", \"SENTTOT\", \"SENTTOT_RAW\"]:\n",
    "        if c in feature_df.columns:\n",
    "            feature_df.drop(columns=[c], inplace=True)\n",
    "\n",
    "    # Treat object/category columns as categorical\n",
    "    cat_cols = [c for c in feature_df.columns if feature_df[c].dtype == \"object\" or str(feature_df[c].dtype) == \"category\"]\n",
    "    encoded_features_df = pd.get_dummies(feature_df, columns=cat_cols, drop_first=True)\n",
    "    print(f\"Fallback encoded features shape: {encoded_features_df.shape}\")\n",
    "\n",
    "# Merge to form final_df\n",
    "# If `SENTTOT_RAW` exists (added in Phase 3), keep it for interpretable units.\n",
    "merge_cols = [\"DISPOSIT\", \"SENTTOT\"]\n",
    "if \"SENTTOT_RAW\" in original_df.columns:\n",
    "    merge_cols.append(\"SENTTOT_RAW\")\n",
    "\n",
    "final_df = encoded_features_df.join(original_df[merge_cols])\n",
    "print(f\"Final merged dataset: {final_df.shape} shape\")\n",
    "\n",
    "print(\"\\nQuick checks on merged data:\")\n",
    "print(final_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUB-PHASE 4.2: ENCODE TARGETS & DATA SPLITS ===\n",
      "\n",
      "--- Classification Target: DISPOSIT ---\n",
      "\n",
      "Binary DISPOSIT distribution:\n",
      "DISPOSIT_BINARY\n",
      "0    74486\n",
      "1     1828\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature matrix shape: (76314, 434)\n",
      "Binary target vector shape: (76314,)\n",
      "\n",
      "Train set shape: (61051, 434) y_train: (61051,)\n",
      "Test set shape: (15263, 434) y_test: (15263,)\n",
      "\n",
      "Binary DISPOSIT Distribution in Train set:\n",
      "DISPOSIT_BINARY\n",
      "0    0.976053\n",
      "1    0.023947\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Binary DISPOSIT Distribution in Test set:\n",
      "DISPOSIT_BINARY\n",
      "0    0.97602\n",
      "1    0.02398\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#   SUB-PHASE 4.2: ENCODE TARGETS \n",
    "#         & DATA SPLITS\n",
    "###############################\n",
    "print(\"\\n=== SUB-PHASE 4.2: ENCODE TARGETS & DATA SPLITS ===\")\n",
    "\n",
    "# --- 4.2.1: DISPOSIT as Classification Target ---\n",
    "print(\"\\n--- Classification Target: DISPOSIT ---\")\n",
    "\n",
    "def binary_disposit(disposit: str) -> int:\n",
    "    \"\"\"Map multi-class disposition into a binary proxy:\n",
    "    1 = went to trial, 0 = did not go to trial.\n",
    "    \"\"\"\n",
    "    if disposit in [\"Jury trial\", \"Trial by judge or bench trial\", \"Guilty plea and trial (>1count)\"]:\n",
    "        return 1\n",
    "    if disposit in [\"Guilty plea\", \"Nolo contendere\"]:\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "final_df[\"DISPOSIT_BINARY\"] = final_df[\"DISPOSIT\"].apply(binary_disposit)\n",
    "print(\"\\nBinary DISPOSIT distribution:\")\n",
    "print(final_df[\"DISPOSIT_BINARY\"].value_counts())\n",
    "\n",
    "# Define X/y\n",
    "X_classif = final_df.drop(columns=[\"DISPOSIT\", \"SENTTOT\", \"DISPOSIT_BINARY\"])\n",
    "y_classif = final_df[\"DISPOSIT_BINARY\"]\n",
    "\n",
    "print(\"\\nFeature matrix shape:\", X_classif.shape)\n",
    "print(\"Binary target vector shape:\", y_classif.shape)\n",
    "\n",
    "# Train/test split with stratification (severe class imbalance)\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_classif,\n",
    "    y_classif,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_classif,\n",
    ")\n",
    "\n",
    "print(\"\\nTrain set shape:\", X_train_c.shape, \"y_train:\", y_train_c.shape)\n",
    "print(\"Test set shape:\", X_test_c.shape, \"y_test:\", y_test_c.shape)\n",
    "\n",
    "print(\"\\nBinary DISPOSIT Distribution in Train set:\")\n",
    "print(y_train_c.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nBinary DISPOSIT Distribution in Test set:\")\n",
    "print(y_test_c.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression Target: SENTTOT_RAW (interpretable units) ---\n",
      "\n",
      "Regression splits:\n",
      "X_train_r: (61051, 433) | y_train_r: (61051,)\n",
      "X_test_r: (15263, 433) | y_test_r: (15263,)\n",
      "\n",
      "SENTTOT_RAW Stats in Train:\n",
      "count    61051.000000\n",
      "mean       208.120914\n",
      "std        543.784218\n",
      "min          0.030000\n",
      "25%          6.000000\n",
      "50%         24.000000\n",
      "75%         78.000000\n",
      "max       2000.000000\n",
      "Name: SENTTOT_RAW, dtype: float64\n",
      "\n",
      "SENTTOT_RAW Stats in Test:\n",
      "count    15263.000000\n",
      "mean       206.918344\n",
      "std        540.800285\n",
      "min          0.030000\n",
      "25%          6.000000\n",
      "50%         24.000000\n",
      "75%         80.000000\n",
      "max       2000.000000\n",
      "Name: SENTTOT_RAW, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 4.2.2: Sentence Length as Regression Target ---\n",
    "# Prefer raw units if available.\n",
    "if \"SENTTOT_RAW\" in final_df.columns:\n",
    "    target_col = \"SENTTOT_RAW\"\n",
    "    print(\"\\n--- Regression Target: SENTTOT_RAW (interpretable units) ---\")\n",
    "else:\n",
    "    target_col = \"SENTTOT\"\n",
    "    print(\"\\n--- Regression Target: SENTTOT (standardized) ---\")\n",
    "\n",
    "final_df[target_col] = pd.to_numeric(final_df[target_col], errors=\"coerce\")\n",
    "\n",
    "# Prepare features/target\n",
    "X_regress = final_df.drop(columns=[\"DISPOSIT\", \"DISPOSIT_BINARY\", \"SENTTOT\"] + ([\"SENTTOT_RAW\"] if \"SENTTOT_RAW\" in final_df.columns else []))\n",
    "y_regress = final_df[target_col]\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_regress,\n",
    "    y_regress,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nRegression splits:\")\n",
    "print(\"X_train_r:\", X_train_r.shape, \"| y_train_r:\", y_train_r.shape)\n",
    "print(\"X_test_r:\", X_test_r.shape, \"| y_test_r:\", y_test_r.shape)\n",
    "\n",
    "print(f\"\\n{target_col} Stats in Train:\")\n",
    "print(y_train_r.describe())\n",
    "print(f\"\\n{target_col} Stats in Test:\")\n",
    "print(y_test_r.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUB-PHASE 4.3: BASELINE CLASSIFICATION (DISPOSIT) ===\n",
      "Train Accuracy (DISPOSIT): 0.696\n",
      "Test Accuracy  (DISPOSIT): 0.688\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.69      0.81     14897\n",
      "           1       0.06      0.78      0.11       366\n",
      "\n",
      "    accuracy                           0.69     15263\n",
      "   macro avg       0.52      0.73      0.46     15263\n",
      "weighted avg       0.97      0.69      0.79     15263\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[10213  4684]\n",
      " [   81   285]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/araj/.pyenv/versions/3.11.8/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#  SUB-PHASE 4.3: BASELINE \n",
    "#      CLASSIFICATION\n",
    "###############################\n",
    "print(\"\\n=== SUB-PHASE 4.3: BASELINE CLASSIFICATION (DISPOSIT) ===\")\n",
    "\n",
    "# 1. Baseline Model with Class Weights & Multinomial\n",
    "model_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',  # Addresses class imbalance\n",
    "    solver='lbfgs'\n",
    ")\n",
    "model_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# 2. Predictions & Evaluation\n",
    "y_pred_c_train = model_clf.predict(X_train_c)\n",
    "y_pred_c_test  = model_clf.predict(X_test_c)\n",
    "\n",
    "# 3. Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_acc_c = accuracy_score(y_train_c, y_pred_c_train)\n",
    "test_acc_c  = accuracy_score(y_test_c, y_pred_c_test)\n",
    "\n",
    "print(f\"Train Accuracy (DISPOSIT): {train_acc_c:.3f}\")\n",
    "print(f\"Test Accuracy  (DISPOSIT): {test_acc_c:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(\n",
    "    y_test_c, y_pred_c_test,\n",
    "    zero_division=0  # to avoid warnings, sets metrics to 0 instead of \"undefined\"\n",
    "))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(confusion_matrix(y_test_c, y_pred_c_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUB-PHASE 4.4: BASELINE REGRESSION (SENTTOT) ===\n",
      "Train MSE (SENTTOT): 229689.860, R2: 0.223\n",
      "Test  MSE (SENTTOT): 233805.901, R2: 0.201\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#   SUB-PHASE 4.4: BASELINE\n",
    "#      REGRESSION\n",
    "###############################\n",
    "print(\"\\n=== SUB-PHASE 4.4: BASELINE REGRESSION (SENTTOT) ===\")\n",
    "\n",
    "# 1. Baseline Model: Linear Regression\n",
    "model_reg = LinearRegression()\n",
    "model_reg.fit(X_train_r, y_train_r)\n",
    "\n",
    "# 2. Predictions & Evaluation\n",
    "y_pred_r_train = model_reg.predict(X_train_r)\n",
    "y_pred_r_test = model_reg.predict(X_test_r)\n",
    "\n",
    "# 3. Metrics: MSE, R2\n",
    "mse_train_r = mean_squared_error(y_train_r, y_pred_r_train)\n",
    "mse_test_r  = mean_squared_error(y_test_r,  y_pred_r_test)\n",
    "\n",
    "r2_train_r = r2_score(y_train_r, y_pred_r_train)\n",
    "r2_test_r  = r2_score(y_test_r,  y_pred_r_test)\n",
    "\n",
    "print(f\"Train MSE (SENTTOT): {mse_train_r:.3f}, R2: {r2_train_r:.3f}\")\n",
    "print(f\"Test  MSE (SENTTOT): {mse_test_r:.3f}, R2: {r2_test_r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group-Level Accuracy for NEWRACE_Black = 1: 0.341\n",
      "Group-Level Accuracy for NEWRACE_Black = 0: 0.777\n",
      "\n",
      "Demographic Parity for 'DISPOSIT' = 1 (assuming label 1 is positive):\n",
      "Group: 0, Positive Rate for label=1: 0.230\n",
      "Group: 1, Positive Rate for label=1: 0.699\n",
      "\n",
      "Protected Group (1) TPR: 0.918, FPR: 0.688\n",
      "Non-Protected Group (0) TPR: 0.685, FPR: 0.221\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Fairness & Group-Level Metrics for Baseline Classification\n",
    "################################################################################\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Identify or Reconstruct Your Protected Attribute\n",
    "# Example: If 'NEWRACE_Black' is a one-hot column in X_test_c:\n",
    "protected_col = 'NEWRACE_Black'  # Adjust for your actual column\n",
    "\n",
    "# 2. Group-Level Accuracy\n",
    "def group_accuracy(X, y_true, y_pred, group_col, group_val):\n",
    "    idx = X[group_col] == group_val\n",
    "    # careful to align indices:\n",
    "    # if X_test_c was reindexed after train_test_split, ensure alignment\n",
    "    return accuracy_score(y_true[idx], y_pred[idx])\n",
    "\n",
    "acc_protected_1 = group_accuracy(X_test_c, y_test_c, y_pred_c_test, protected_col, 1)\n",
    "acc_protected_0 = group_accuracy(X_test_c, y_test_c, y_pred_c_test, protected_col, 0)\n",
    "\n",
    "print(f\"\\nGroup-Level Accuracy for {protected_col} = 1: {acc_protected_1:.3f}\")\n",
    "print(f\"Group-Level Accuracy for {protected_col} = 0: {acc_protected_0:.3f}\")\n",
    "\n",
    "# 3. Demographic Parity (Positive Prediction Rates)\n",
    "# This is relevant if you define \"positive\" = predicted class is 1\n",
    "# For a multi-class scenario, you might pick one class (e.g., \"Guilty\" if encoded as 1).\n",
    "def demographic_parity(X, y_pred, group_col, positive_label=1):\n",
    "    groups = [0,1]  # if your one-hot col is 0 or 1\n",
    "    for g in groups:\n",
    "        idx = X[group_col] == g\n",
    "        # Probability model assigned label=positive_label in that group\n",
    "        pos_rate = np.mean((y_pred[idx] == positive_label))\n",
    "        print(f\"Group: {g}, Positive Rate for label={positive_label}: {pos_rate:.3f}\")\n",
    "\n",
    "print(\"\\nDemographic Parity for 'DISPOSIT' = 1 (assuming label 1 is positive):\")\n",
    "demographic_parity(X_test_c, y_pred_c_test, protected_col, positive_label=1)\n",
    "\n",
    "# 4. TPR/FPR for each group (Binary scenario)\n",
    "# If your DISPOSIT is truly multi-class, you'd have to define a \"positive\" class vs. \"all else.\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_tpr_fpr(y_true, y_pred, positive_label=1):\n",
    "    # For simplicity, treat everything that isn't 'positive_label' as 0\n",
    "    # This is only valid if your classification is effectively binary\n",
    "    y_true_bin = (y_true == positive_label).astype(int)\n",
    "    y_pred_bin = (y_pred == positive_label).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    return tpr, fpr\n",
    "\n",
    "# Example usage:\n",
    "tpr_1, fpr_1 = compute_tpr_fpr(\n",
    "    y_test_c[X_test_c[protected_col] == 1], \n",
    "    y_pred_c_test[X_test_c[protected_col] == 1],\n",
    "    positive_label=1\n",
    ")\n",
    "tpr_0, fpr_0 = compute_tpr_fpr(\n",
    "    y_test_c[X_test_c[protected_col] == 0], \n",
    "    y_pred_c_test[X_test_c[protected_col] == 0],\n",
    "    positive_label=1\n",
    ")\n",
    "\n",
    "print(f\"\\nProtected Group (1) TPR: {tpr_1:.3f}, FPR: {fpr_1:.3f}\")\n",
    "print(f\"Non-Protected Group (0) TPR: {tpr_0:.3f}, FPR: {fpr_0:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
