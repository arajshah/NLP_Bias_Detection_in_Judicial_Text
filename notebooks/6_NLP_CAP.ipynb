{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/araj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/araj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Paths\n",
    "# -----------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"cap_data\").exists() and (PROJECT_ROOT.parent / \"cap_data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "CAP_DIR = PROJECT_ROOT / \"cap_data\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NLTK resources (safe if already downloaded)\n",
    "# -----------------------------------------------------------------------------\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# spaCy model: do NOT download inside notebook by default (non-reproducible).\n",
    "# If you need it, install it in your environment and then load.\n",
    "# -----------------------------------------------------------------------------\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    nlp = None\n",
    "    print(\"WARNING: spaCy model 'en_core_web_sm' not available.\")\n",
    "    print(\"Install with: python -m spacy download en_core_web_sm\")\n",
    "    print(\"Underlying error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata rows: 70\n",
      "{'__source__': 'VolumeMetadata.json', 'volume_number': '219', 'title': None, 'publisher': None, 'publication_year': None, 'start_year': 2017, 'end_year': 2017, 'series_volume_number': None, 'jurisdictions': [{'id': 30, 'name': 'Cal.', 'name_long': 'California'}], 'id': 'CalRptr3d_219', 'harvard_hollis_id': None, 'spine_start_year': 2017, 'spine_end_year': 2017, 'publication_city': None, 'second_part_of_id': None, 'redacted': False, 'nominative_reporter': None}\n",
      "            __source__ volume_number  title  publisher  publication_year  \\\n",
      "0  VolumeMetadata.json           219    NaN        NaN               NaN   \n",
      "1   CasesMetadata.json           NaN    NaN        NaN               NaN   \n",
      "2   CasesMetadata.json           NaN    NaN        NaN               NaN   \n",
      "\n",
      "   start_year  end_year  series_volume_number  \\\n",
      "0      2017.0    2017.0                   NaN   \n",
      "1         NaN       NaN                   NaN   \n",
      "2         NaN       NaN                   NaN   \n",
      "\n",
      "                                       jurisdictions             id  ...  \\\n",
      "0  [{'id': 30, 'name': 'Cal.', 'name_long': 'Cali...  CalRptr3d_219  ...   \n",
      "1                                                NaN       12513529  ...   \n",
      "2                                                NaN       12513530  ...   \n",
      "\n",
      "   analysis.cardinality  analysis.char_count  analysis.pagerank.raw  \\\n",
      "0                   NaN                  NaN                    NaN   \n",
      "1                 787.0              18684.0           4.035808e-08   \n",
      "2                1711.0              54463.0           7.285885e-08   \n",
      "\n",
      "   analysis.pagerank.percentile  \\\n",
      "0                           NaN   \n",
      "1                      0.258653   \n",
      "2                      0.433566   \n",
      "\n",
      "                                     analysis.sha256    analysis.simhash  \\\n",
      "0                                                NaN                 NaN   \n",
      "1  ef0e6ed778424c4928b03fe6769527466d68b6e58b7696...  1:e8a002217a768f6d   \n",
      "2  5d26545148d670338c530b5288bc3a0a520ce3fe09babd...  1:edfe692740e407e8   \n",
      "\n",
      "   analysis.word_count provenance.date_added provenance.source  \\\n",
      "0                  NaN                   NaN               NaN   \n",
      "1               2951.0            2021-08-27          Fastcase   \n",
      "2               9115.0            2021-08-27          Fastcase   \n",
      "\n",
      "  provenance.batch  \n",
      "0              NaN  \n",
      "1             2021  \n",
      "2             2021  \n",
      "\n",
      "[3 rows x 45 columns]\n",
      "Metadata columns: 45\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# CAP metadata lives under `cap_data/metadata/`\n",
    "metadata_dir = CAP_DIR / \"metadata\"\n",
    "\n",
    "# In this repo, `CasesMetadata.json` is a *list* of dicts.\n",
    "cases_metadata_path = metadata_dir / \"CasesMetadata.json\"\n",
    "volume_metadata_path = metadata_dir / \"VolumeMetadata.json\"\n",
    "\n",
    "metadata_rows = []\n",
    "\n",
    "# Volume metadata (dict)\n",
    "if volume_metadata_path.exists():\n",
    "    with open(volume_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        volume_meta = json.load(f)\n",
    "    # Normalize into a single row\n",
    "    volume_row = {\"__source__\": \"VolumeMetadata.json\", **volume_meta}\n",
    "    metadata_rows.append(volume_row)\n",
    "\n",
    "# Cases metadata (list[dict])\n",
    "if cases_metadata_path.exists():\n",
    "    with open(cases_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cases_meta = json.load(f)\n",
    "\n",
    "    if isinstance(cases_meta, list):\n",
    "        for row in cases_meta:\n",
    "            if isinstance(row, dict):\n",
    "                metadata_rows.append({\"__source__\": \"CasesMetadata.json\", **row})\n",
    "    elif isinstance(cases_meta, dict):\n",
    "        metadata_rows.append({\"__source__\": \"CasesMetadata.json\", **cases_meta})\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected JSON type for {cases_metadata_path}: {type(cases_meta)}\")\n",
    "\n",
    "print(f\"Loaded metadata rows: {len(metadata_rows)}\")\n",
    "print(metadata_rows[0] if metadata_rows else \"<no metadata loaded>\")\n",
    "\n",
    "# Flatten metadata\n",
    "if metadata_rows:\n",
    "    df_metadata = pd.json_normalize(metadata_rows)\n",
    "    print(df_metadata.head(3))\n",
    "    print(\"Metadata columns:\", len(df_metadata.columns))\n",
    "else:\n",
    "    df_metadata = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_dir = CAP_DIR / \"html\"\n",
    "\n",
    "def extract_case_text_from_cap_html(html: str) -> str:\n",
    "    \"\"\"Extract opinion text from CAP HTML.\n",
    "\n",
    "    CAP HTML in this repo uses:\n",
    "    - <article class=\"opinion\"> ... <p> ... </p>\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Prefer opinion article; fallback to whole casebody\n",
    "    opinion = soup.select_one(\"article.opinion\")\n",
    "    scope = opinion if opinion is not None else soup.select_one(\"section.casebody\")\n",
    "    if scope is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove footnotes to reduce noise\n",
    "    for foot in scope.select(\"aside.footnote\"):\n",
    "        foot.decompose()\n",
    "\n",
    "    paras = [p.get_text(\" \", strip=True) for p in scope.select(\"p\")]\n",
    "    text = \"\\n\".join([t for t in paras if t])\n",
    "    return text\n",
    "\n",
    "rows = []\n",
    "\n",
    "if not html_dir.exists():\n",
    "    raise FileNotFoundError(f\"CAP HTML dir not found: {html_dir}\")\n",
    "\n",
    "for path in sorted(html_dir.glob(\"*.html\")):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    case_id = path.stem  # e.g. 0065-01\n",
    "    text = extract_case_text_from_cap_html(html)\n",
    "\n",
    "    # Pull a title if present\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    parties = soup.select_one(\"section.head-matter p.parties\")\n",
    "    title = parties.get_text(\" \", strip=True) if parties else None\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"case_id\": case_id,\n",
    "            \"title\": title,\n",
    "            \"opinion_text\": text,\n",
    "            \"opinion_char_len\": len(text),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_text = pd.DataFrame(rows)\n",
    "print(df_text.head(3))\n",
    "print(\"Extracted cases:\", len(df_text))\n",
    "print(\"Empty texts:\", (df_text[\"opinion_char_len\"] == 0).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Merge CAP JSON metadata onto extracted text + export `data/legal_text_data.csv`\n",
    "# -----------------------------------------------------------------------------\n",
    "import json\n",
    "\n",
    "json_dir = CAP_DIR / \"json\"\n",
    "\n",
    "meta_rows = []\n",
    "for path in sorted(json_dir.glob(\"*.json\")):\n",
    "    case_id = path.stem  # matches html stem\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    # Extract a compact, stable schema\n",
    "    citations = obj.get(\"citations\") or []\n",
    "    official_cite = None\n",
    "    for c in citations:\n",
    "        if isinstance(c, dict) and c.get(\"type\") == \"official\":\n",
    "            official_cite = c.get(\"cite\")\n",
    "            break\n",
    "\n",
    "    court = obj.get(\"court\") or {}\n",
    "    juris = obj.get(\"jurisdiction\") or {}\n",
    "\n",
    "    meta_rows.append(\n",
    "        {\n",
    "            \"case_id\": case_id,\n",
    "            \"cap_case_numeric_id\": obj.get(\"id\"),\n",
    "            \"name\": obj.get(\"name\"),\n",
    "            \"name_abbreviation\": obj.get(\"name_abbreviation\"),\n",
    "            \"decision_date\": obj.get(\"decision_date\"),\n",
    "            \"docket_number\": obj.get(\"docket_number\"),\n",
    "            \"first_page\": obj.get(\"first_page\"),\n",
    "            \"last_page\": obj.get(\"last_page\"),\n",
    "            \"official_citation\": official_cite,\n",
    "            \"court_name\": court.get(\"name\"),\n",
    "            \"court_abbrev\": court.get(\"name_abbreviation\"),\n",
    "            \"jurisdiction\": juris.get(\"name_long\") or juris.get(\"name\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_cap_meta = pd.DataFrame(meta_rows)\n",
    "print(\"CAP JSON meta rows:\", len(df_cap_meta))\n",
    "print(df_cap_meta.head(3))\n",
    "\n",
    "# Join meta onto text\n",
    "legal_df = df_text.merge(df_cap_meta, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Bias proxy features (NOT ground-truth labels)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This is a *weak heuristic* to give you something to iterate on.\n",
    "# Treat it as a feature, not a definitive \"bias\" label.\n",
    "BIAS_TERMS = [\n",
    "    # race/ethnicity\n",
    "    \"black\",\n",
    "    \"white\",\n",
    "    \"hispanic\",\n",
    "    \"latino\",\n",
    "    \"asian\",\n",
    "    \"native\",\n",
    "    \"indian\",\n",
    "    \"race\",\n",
    "    \"racial\",\n",
    "    \"ethnicity\",\n",
    "    # gender\n",
    "    \"male\",\n",
    "    \"female\",\n",
    "    \"woman\",\n",
    "    \"women\",\n",
    "    \"man\",\n",
    "    \"men\",\n",
    "    \"gender\",\n",
    "    # citizenship/immigration\n",
    "    \"alien\",\n",
    "    \"immigrant\",\n",
    "    \"immigration\",\n",
    "    \"citizen\",\n",
    "    \"citizenship\",\n",
    "    \"deport\",\n",
    "    \"deportation\",\n",
    "]\n",
    "\n",
    "def bias_proxy_hits(text: str) -> int:\n",
    "    t = (text or \"\").lower()\n",
    "    return sum(t.count(term) for term in BIAS_TERMS)\n",
    "\n",
    "legal_df[\"bias_proxy_term_hits\"] = legal_df[\"opinion_text\"].astype(str).apply(bias_proxy_hits)\n",
    "legal_df[\"BIAS_LABEL\"] = (legal_df[\"bias_proxy_term_hits\"] > 0).astype(int)\n",
    "\n",
    "# Export\n",
    "out_path = PROJECT_ROOT / \"data\" / \"legal_text_data.csv\"\n",
    "legal_df.to_csv(out_path, index=False)\n",
    "print(f\"Wrote: {out_path} | shape={legal_df.shape}\")\n",
    "\n",
    "# Preview\n",
    "print(legal_df[[\"case_id\", \"decision_date\", \"court_abbrev\", \"opinion_char_len\", \"bias_proxy_term_hits\", \"BIAS_LABEL\"]].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Remove citations and special characters\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)  # Remove [1], [2], etc.\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)  # Remove (1), (2), etc.\n",
    "    text = re.sub(r\"\\*\\d+\", \"\", text)  # Remove *123\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_text[\"clean_opinion\"] = df_text[\"opinion_text\"].astype(str).apply(preprocess_text)\n",
    "print(df_text[[\"case_id\", \"clean_opinion\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_score(text: str) -> float:\n",
    "    return sid.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "df_text[\"sentiment_score\"] = df_text[\"clean_opinion\"].astype(str).apply(sentiment_score)\n",
    "print(df_text[[\"case_id\", \"sentiment_score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df_text['clean_opinion'])\n",
    "\n",
    "# Define number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display topics\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Topic #{index + 1}:')\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
