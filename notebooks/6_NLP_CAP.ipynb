{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/araj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/araj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Flatten the jurisdictions field if it's a simple list\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjurisdictions\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     19\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjurisdictions_flat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjurisdictions\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     21\u001b[0m metadata_list\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the METADATA folder\n",
    "metadata_path = '../cap_data/METADATA'\n",
    "\n",
    "# Initialize an empty list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "# Iterate through each JSON file in the METADATA folder\n",
    "for file in os.listdir(metadata_path):\n",
    "    if file.endswith('.json'):\n",
    "        with open(os.path.join(metadata_path, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Flatten the jurisdictions field if it's a simple list\n",
    "            if isinstance(data.get('jurisdictions'), list):\n",
    "                data['jurisdictions_flat'] = ', '.join(map(str, data['jurisdictions']))\n",
    "            \n",
    "            metadata_list.append(data)\n",
    "\n",
    "# Check the first metadata record\n",
    "print(metadata_list[0])\n",
    "\n",
    "# Convert the list of metadata to a DataFrame\n",
    "df_metadata = pd.json_normalize(metadata_list)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(df_metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Path to the HTML folder\n",
    "html_path = 'path_to_downloaded_data/HTML/'\n",
    "\n",
    "# Initialize lists to store case names and their corresponding text\n",
    "case_names = []\n",
    "case_texts = []\n",
    "\n",
    "# Iterate through each HTML file in the HTML folder\n",
    "for file in os.listdir(html_path):\n",
    "    if file.endswith('.html'):\n",
    "        with open(os.path.join(html_path, file), 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            # Extract case name from metadata (assuming filename matches)\n",
    "            case_name = file.replace('.html', '')\n",
    "            case_names.append(case_name)\n",
    "            # Extract main text of the opinion\n",
    "            # This may vary based on HTML structure; adjust selectors as needed\n",
    "            opinion = soup.find('div', class_='opinion-text')  # Example selector\n",
    "            if opinion:\n",
    "                case_texts.append(opinion.get_text(separator=' ', strip=True))\n",
    "            else:\n",
    "                case_texts.append('')\n",
    "\n",
    "# Create a DataFrame with case names and texts\n",
    "df_text = pd.DataFrame({\n",
    "    'case_name': case_names,\n",
    "    'opinion_text': case_texts\n",
    "})\n",
    "\n",
    "print(df_text.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove citations and special characters\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove [1], [2], etc.\n",
    "    text = re.sub(r'\\(\\d+\\)', '', text)  # Remove (1), (2), etc.\n",
    "    text = re.sub(r'\\*\\d+', '', text)    # Remove *123\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Join back to string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_text['clean_opinion'] = df_text['opinion_text'].apply(preprocess_text)\n",
    "print(df_text[['case_name', 'clean_opinion']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_score(text):\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "df_text['sentiment_score'] = df_text['clean_opinion'].apply(sentiment_score)\n",
    "print(df_text[['case_name', 'sentiment_score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df_text['clean_opinion'])\n",
    "\n",
    "# Define number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display topics\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Topic #{index + 1}:')\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
