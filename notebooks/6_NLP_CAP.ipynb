{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/araj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/araj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Paths\n",
    "# -----------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"cap_data\").exists() and (PROJECT_ROOT.parent / \"cap_data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "CAP_DIR = PROJECT_ROOT / \"cap_data\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NLTK resources (safe if already downloaded)\n",
    "# -----------------------------------------------------------------------------\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# spaCy model: do NOT download inside notebook by default (non-reproducible).\n",
    "# If you need it, install it in your environment and then load.\n",
    "# -----------------------------------------------------------------------------\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    nlp = None\n",
    "    print(\"WARNING: spaCy model 'en_core_web_sm' not available.\")\n",
    "    print(\"Install with: python -m spacy download en_core_web_sm\")\n",
    "    print(\"Underlying error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Flatten the jurisdictions field if it's a simple list\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjurisdictions\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     19\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjurisdictions_flat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjurisdictions\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     21\u001b[0m metadata_list\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# CAP metadata lives under `cap_data/metadata/`\n",
    "metadata_dir = CAP_DIR / \"metadata\"\n",
    "\n",
    "# In this repo, `CasesMetadata.json` is a *list* of dicts.\n",
    "cases_metadata_path = metadata_dir / \"CasesMetadata.json\"\n",
    "volume_metadata_path = metadata_dir / \"VolumeMetadata.json\"\n",
    "\n",
    "metadata_rows = []\n",
    "\n",
    "# Volume metadata (dict)\n",
    "if volume_metadata_path.exists():\n",
    "    with open(volume_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        volume_meta = json.load(f)\n",
    "    # Normalize into a single row\n",
    "    volume_row = {\"__source__\": \"VolumeMetadata.json\", **volume_meta}\n",
    "    metadata_rows.append(volume_row)\n",
    "\n",
    "# Cases metadata (list[dict])\n",
    "if cases_metadata_path.exists():\n",
    "    with open(cases_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cases_meta = json.load(f)\n",
    "\n",
    "    if isinstance(cases_meta, list):\n",
    "        for row in cases_meta:\n",
    "            if isinstance(row, dict):\n",
    "                metadata_rows.append({\"__source__\": \"CasesMetadata.json\", **row})\n",
    "    elif isinstance(cases_meta, dict):\n",
    "        metadata_rows.append({\"__source__\": \"CasesMetadata.json\", **cases_meta})\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected JSON type for {cases_metadata_path}: {type(cases_meta)}\")\n",
    "\n",
    "print(f\"Loaded metadata rows: {len(metadata_rows)}\")\n",
    "print(metadata_rows[0] if metadata_rows else \"<no metadata loaded>\")\n",
    "\n",
    "# Flatten metadata\n",
    "if metadata_rows:\n",
    "    df_metadata = pd.json_normalize(metadata_rows)\n",
    "    print(df_metadata.head(3))\n",
    "    print(\"Metadata columns:\", len(df_metadata.columns))\n",
    "else:\n",
    "    df_metadata = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_dir = CAP_DIR / \"html\"\n",
    "\n",
    "def extract_case_text_from_cap_html(html: str) -> str:\n",
    "    \"\"\"Extract opinion text from CAP HTML.\n",
    "\n",
    "    CAP HTML in this repo uses:\n",
    "    - <article class=\"opinion\"> ... <p> ... </p>\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Prefer opinion article; fallback to whole casebody\n",
    "    opinion = soup.select_one(\"article.opinion\")\n",
    "    scope = opinion if opinion is not None else soup.select_one(\"section.casebody\")\n",
    "    if scope is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove footnotes to reduce noise\n",
    "    for foot in scope.select(\"aside.footnote\"):\n",
    "        foot.decompose()\n",
    "\n",
    "    paras = [p.get_text(\" \", strip=True) for p in scope.select(\"p\")]\n",
    "    text = \"\\n\".join([t for t in paras if t])\n",
    "    return text\n",
    "\n",
    "rows = []\n",
    "\n",
    "if not html_dir.exists():\n",
    "    raise FileNotFoundError(f\"CAP HTML dir not found: {html_dir}\")\n",
    "\n",
    "for path in sorted(html_dir.glob(\"*.html\")):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    case_id = path.stem  # e.g. 0065-01\n",
    "    text = extract_case_text_from_cap_html(html)\n",
    "\n",
    "    # Pull a title if present\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    parties = soup.select_one(\"section.head-matter p.parties\")\n",
    "    title = parties.get_text(\" \", strip=True) if parties else None\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"case_id\": case_id,\n",
    "            \"title\": title,\n",
    "            \"opinion_text\": text,\n",
    "            \"opinion_char_len\": len(text),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_text = pd.DataFrame(rows)\n",
    "print(df_text.head(3))\n",
    "print(\"Extracted cases:\", len(df_text))\n",
    "print(\"Empty texts:\", (df_text[\"opinion_char_len\"] == 0).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Merge CAP JSON metadata onto extracted text + export `data/legal_text_data.csv`\n",
    "# -----------------------------------------------------------------------------\n",
    "import json\n",
    "\n",
    "json_dir = CAP_DIR / \"json\"\n",
    "\n",
    "meta_rows = []\n",
    "for path in sorted(json_dir.glob(\"*.json\")):\n",
    "    case_id = path.stem  # matches html stem\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    # Extract a compact, stable schema\n",
    "    citations = obj.get(\"citations\") or []\n",
    "    official_cite = None\n",
    "    for c in citations:\n",
    "        if isinstance(c, dict) and c.get(\"type\") == \"official\":\n",
    "            official_cite = c.get(\"cite\")\n",
    "            break\n",
    "\n",
    "    court = obj.get(\"court\") or {}\n",
    "    juris = obj.get(\"jurisdiction\") or {}\n",
    "\n",
    "    meta_rows.append(\n",
    "        {\n",
    "            \"case_id\": case_id,\n",
    "            \"cap_case_numeric_id\": obj.get(\"id\"),\n",
    "            \"name\": obj.get(\"name\"),\n",
    "            \"name_abbreviation\": obj.get(\"name_abbreviation\"),\n",
    "            \"decision_date\": obj.get(\"decision_date\"),\n",
    "            \"docket_number\": obj.get(\"docket_number\"),\n",
    "            \"first_page\": obj.get(\"first_page\"),\n",
    "            \"last_page\": obj.get(\"last_page\"),\n",
    "            \"official_citation\": official_cite,\n",
    "            \"court_name\": court.get(\"name\"),\n",
    "            \"court_abbrev\": court.get(\"name_abbreviation\"),\n",
    "            \"jurisdiction\": juris.get(\"name_long\") or juris.get(\"name\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_cap_meta = pd.DataFrame(meta_rows)\n",
    "print(\"CAP JSON meta rows:\", len(df_cap_meta))\n",
    "print(df_cap_meta.head(3))\n",
    "\n",
    "# Join meta onto text\n",
    "legal_df = df_text.merge(df_cap_meta, on=\"case_id\", how=\"left\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Bias proxy features (NOT ground-truth labels)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This is a *weak heuristic* to give you something to iterate on.\n",
    "# Treat it as a feature, not a definitive \"bias\" label.\n",
    "BIAS_TERMS = [\n",
    "    # race/ethnicity\n",
    "    \"black\",\n",
    "    \"white\",\n",
    "    \"hispanic\",\n",
    "    \"latino\",\n",
    "    \"asian\",\n",
    "    \"native\",\n",
    "    \"indian\",\n",
    "    \"race\",\n",
    "    \"racial\",\n",
    "    \"ethnicity\",\n",
    "    # gender\n",
    "    \"male\",\n",
    "    \"female\",\n",
    "    \"woman\",\n",
    "    \"women\",\n",
    "    \"man\",\n",
    "    \"men\",\n",
    "    \"gender\",\n",
    "    # citizenship/immigration\n",
    "    \"alien\",\n",
    "    \"immigrant\",\n",
    "    \"immigration\",\n",
    "    \"citizen\",\n",
    "    \"citizenship\",\n",
    "    \"deport\",\n",
    "    \"deportation\",\n",
    "]\n",
    "\n",
    "def bias_proxy_hits(text: str) -> int:\n",
    "    t = (text or \"\").lower()\n",
    "    return sum(t.count(term) for term in BIAS_TERMS)\n",
    "\n",
    "legal_df[\"bias_proxy_term_hits\"] = legal_df[\"opinion_text\"].astype(str).apply(bias_proxy_hits)\n",
    "legal_df[\"BIAS_LABEL\"] = (legal_df[\"bias_proxy_term_hits\"] > 0).astype(int)\n",
    "\n",
    "# Export\n",
    "out_path = PROJECT_ROOT / \"data\" / \"legal_text_data.csv\"\n",
    "legal_df.to_csv(out_path, index=False)\n",
    "print(f\"Wrote: {out_path} | shape={legal_df.shape}\")\n",
    "\n",
    "# Preview\n",
    "print(legal_df[[\"case_id\", \"decision_date\", \"court_abbrev\", \"opinion_char_len\", \"bias_proxy_term_hits\", \"BIAS_LABEL\"]].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Remove citations and special characters\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)  # Remove [1], [2], etc.\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)  # Remove (1), (2), etc.\n",
    "    text = re.sub(r\"\\*\\d+\", \"\", text)  # Remove *123\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_text[\"clean_opinion\"] = df_text[\"opinion_text\"].astype(str).apply(preprocess_text)\n",
    "print(df_text[[\"case_id\", \"clean_opinion\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_score(text: str) -> float:\n",
    "    return sid.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "df_text[\"sentiment_score\"] = df_text[\"clean_opinion\"].astype(str).apply(sentiment_score)\n",
    "print(df_text[[\"case_id\", \"sentiment_score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df_text['clean_opinion'])\n",
    "\n",
    "# Define number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Initialize LDA\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display topics\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f'Topic #{index + 1}:')\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
