{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (76538, 16)\n",
      "Dropped 224 duplicate rows\n",
      "\n",
      "Data Types:\n",
      "AGE         float64\n",
      "NEWRACE      object\n",
      "MONSEX       object\n",
      "EDUCATN      object\n",
      "DISTRICT     object\n",
      "CIRCDIST     object\n",
      "CRIMHIST     object\n",
      "SENTYR      float64\n",
      "CITIZEN      object\n",
      "CITWHERE     object\n",
      "NUMDEPEN     object\n",
      "CRIMLIV      object\n",
      "SENTMON      object\n",
      "ZONE         object\n",
      "DISPOSIT     object\n",
      "SENTTOT     float64\n",
      "dtype: object\n",
      "\n",
      "Unique values in NEWRACE:\n",
      "['Hispanic' 'White' 'Black' 'Other' 'American Indian or Alaskan Native'\n",
      " 'Asian or Pacific Islander' nan]\n",
      "\n",
      "Unique values in MONSEX:\n",
      "['Male' 'Female' nan]\n",
      "\n",
      "Unique values in EDUCATN:\n",
      "['Six years of school completed' 'High school graduate'\n",
      " 'Some trade or vocational school' 'Nine years of school completed'\n",
      " 'Some college' 'College graduate' 'Eleven years of school completed'\n",
      " 'G.E.D. (general education diploma)' nan 'One year of school completed'\n",
      " 'Trade or vocational degree' 'Middle school / junior high'\n",
      " 'Ten years of school completed' 'Four years of school completed'\n",
      " 'Some high school' 'Associate degree (A.A.)'\n",
      " 'Eight years of school completed'\n",
      " 'Graduate degree (MST, J.D., M.D., PH.D., etc)'\n",
      " 'Seven years of school completed' 'Three years of school completed'\n",
      " 'Two years of school completed' 'Five years of school completed'\n",
      " 'No schooling' 'Some elementary school' 'Military training'\n",
      " 'One year of college / freshman' 'Two years of college / sophomore'\n",
      " 'Three years of college / junior' 'Some graduate school']\n",
      "\n",
      "Unique values in CITIZEN:\n",
      "['Illegal alien' 'United States citizen' 'Resident/legal alien'\n",
      " 'Not a US citizen/alien status unknown' nan 'Extradited Alien']\n",
      "\n",
      "Unique values in ZONE:\n",
      "['Zone D' 'Zone A' nan 'Zone B' 'Zone C']\n",
      "\n",
      "Unique values in SENTMON:\n",
      "['October' 'November' 'December' 'January' 'February' 'March' 'April'\n",
      " 'May' 'June' 'July' 'August' 'September']\n",
      "\n",
      "Unique values in DISPOSIT:\n",
      "['Guilty plea' 'Jury trial' 'Nolo contendere'\n",
      " 'Trial by judge or bench trial' 'Guilty plea and trial (>1count)']\n",
      "\n",
      "Missing values (top 10):\n",
      "NUMDEPEN    8317\n",
      "EDUCATN     8296\n",
      "SENTTOT     6336\n",
      "CRIMLIV     4994\n",
      "CRIMHIST    1968\n",
      "CITWHERE    1352\n",
      "ZONE        1145\n",
      "NEWRACE      794\n",
      "CITIZEN      415\n",
      "MONSEX       104\n",
      "dtype: int64\n",
      "\n",
      "Intermediate dataset saved to: /Users/araj/Documents/Code/Machine Learning/Bias Detection in Judicial Text /data/intermediate/cleaned_data_after_subphase_3_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Robust project-root + stable intermediate/output paths\n",
    "# -----------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "INTERMEDIATE_DIR = DATA_DIR / \"intermediate\"\n",
    "INTERMEDIATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "phase2_path = DATA_DIR / \"cleaned_data_phase2.csv\"\n",
    "ussc_df = pd.read_csv(phase2_path, low_memory=False)\n",
    "\n",
    "# Step 1: Confirm Data Integrity\n",
    "print(\"Dataset shape:\", ussc_df.shape)\n",
    "\n",
    "# Step 2: Remove Duplicates\n",
    "before_dups = ussc_df.shape[0]\n",
    "ussc_df.drop_duplicates(inplace=True)\n",
    "after_dups = ussc_df.shape[0]\n",
    "print(f\"Dropped {before_dups - after_dups} duplicate rows\")\n",
    "\n",
    "# Step 3: Check Data Types\n",
    "print(\"\\nData Types:\")\n",
    "print(ussc_df.dtypes)\n",
    "\n",
    "# Step 4: Validate Unique Values in Key Categorical Features\n",
    "categorical_features = [\n",
    "    \"NEWRACE\",\n",
    "    \"MONSEX\",\n",
    "    \"EDUCATN\",\n",
    "    \"CITIZEN\",\n",
    "    \"ZONE\",\n",
    "    \"SENTMON\",\n",
    "    \"DISPOSIT\",\n",
    "]\n",
    "for feature in categorical_features:\n",
    "    if feature in ussc_df.columns:\n",
    "        print(f\"\\nUnique values in {feature}:\")\n",
    "        print(ussc_df[feature].unique())\n",
    "\n",
    "# Step 5: Check for Missing Data\n",
    "missing_counts = ussc_df.isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values (top 10):\")\n",
    "print(missing_counts.head(10))\n",
    "\n",
    "# Step 6: Save intermediate dataset\n",
    "output_file = INTERMEDIATE_DIR / \"cleaned_data_after_subphase_3_1.csv\"\n",
    "ussc_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nIntermediate dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values per Feature:\n",
      "AGE           44\n",
      "NEWRACE      794\n",
      "MONSEX       104\n",
      "EDUCATN     8296\n",
      "DISTRICT       0\n",
      "CIRCDIST       0\n",
      "CRIMHIST    1968\n",
      "SENTYR         0\n",
      "CITIZEN      415\n",
      "CITWHERE    1352\n",
      "NUMDEPEN    8317\n",
      "CRIMLIV     4994\n",
      "SENTMON        0\n",
      "ZONE        1145\n",
      "DISPOSIT       0\n",
      "SENTTOT     6336\n",
      "dtype: int64\n",
      "count    76314.000000\n",
      "mean       207.880397\n",
      "std        543.185412\n",
      "min          0.030000\n",
      "25%          6.000000\n",
      "50%         24.000000\n",
      "75%         80.000000\n",
      "max       2000.000000\n",
      "Name: SENTTOT, dtype: float64\n",
      "\n",
      "Data Types:\n",
      "AGE         float64\n",
      "NEWRACE      object\n",
      "MONSEX       object\n",
      "EDUCATN      object\n",
      "DISTRICT     object\n",
      "CIRCDIST     object\n",
      "CRIMHIST      int64\n",
      "SENTYR      float64\n",
      "CITIZEN      object\n",
      "CITWHERE     object\n",
      "NUMDEPEN    float64\n",
      "CRIMLIV      object\n",
      "SENTMON      object\n",
      "ZONE         object\n",
      "DISPOSIT     object\n",
      "SENTTOT     float64\n",
      "dtype: object\n",
      "Filled missing values in AGE with median: 35.0\n",
      "Filled missing values in NUMDEPEN with median: 1.0\n",
      "Filled missing values in CRIMHIST with median: 1.0\n",
      "Filled missing values in SENTYR with median: 2019.0\n",
      "Filled missing values in SENTTOT with median: 24.0\n",
      "\n",
      "Missing Values After Handling:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Dataset after handling missing values saved to: /Users/araj/Documents/Code/Machine Learning/Bias Detection in Judicial Text /data/intermediate/cleaned_data_after_subphase_3_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the intermediate dataset from Sub-Phase 3.1\n",
    "file_path = INTERMEDIATE_DIR / \"cleaned_data_after_subphase_3_1.csv\"\n",
    "ussc_df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Step 1: Check for Missing Values Again\n",
    "missing_counts = ussc_df.isna().sum()\n",
    "print(\"\\nMissing Values per Feature:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Step 2: Preprocessing Specific Features\n",
    "# Convert \"No dependents\" in NUMDEPEN to 0 and ensure numeric type\n",
    "if \"NUMDEPEN\" in ussc_df.columns:\n",
    "    ussc_df[\"NUMDEPEN\"] = ussc_df[\"NUMDEPEN\"].replace(\"No dependents\", 0).astype(float)\n",
    "\n",
    "# Convert CRIMHIST to binary values: 1 for \"Yes, there is a criminal history\", 0 otherwise\n",
    "if \"CRIMHIST\" in ussc_df.columns:\n",
    "    ussc_df[\"CRIMHIST\"] = ussc_df[\"CRIMHIST\"].apply(\n",
    "        lambda x: 1 if x == \"Yes, there is a criminal history\" else 0\n",
    "    )\n",
    "\n",
    "# Cap extreme outliers in SENTTOT (documented heuristic)\n",
    "if \"SENTTOT\" in ussc_df.columns:\n",
    "    ussc_df[\"SENTTOT\"] = ussc_df[\"SENTTOT\"].apply(lambda x: x if x < 2000 else 2000)\n",
    "    print(ussc_df[\"SENTTOT\"].describe())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(ussc_df.dtypes)\n",
    "\n",
    "# Step 3: Handling Missing Values\n",
    "numerical_features = [c for c in [\"AGE\", \"NUMDEPEN\", \"CRIMHIST\", \"SENTYR\", \"SENTTOT\"] if c in ussc_df.columns]\n",
    "for feature in numerical_features:\n",
    "    median_value = ussc_df[feature].median()\n",
    "    ussc_df.fillna({feature: median_value}, inplace=True)\n",
    "    print(f\"Filled missing values in {feature} with median: {median_value}\")\n",
    "\n",
    "categorical_features = [\n",
    "    c for c in [\n",
    "        \"NEWRACE\",\n",
    "        \"MONSEX\",\n",
    "        \"EDUCATN\",\n",
    "        \"CITIZEN\",\n",
    "        \"CITWHERE\",\n",
    "        \"ZONE\",\n",
    "        \"DISTRICT\",\n",
    "        \"CIRCDIST\",\n",
    "        \"CRIMLIV\",\n",
    "        \"SENTMON\",\n",
    "    ]\n",
    "    if c in ussc_df.columns\n",
    "]\n",
    "for feature in categorical_features:\n",
    "    ussc_df.fillna({feature: \"Unknown\"}, inplace=True)\n",
    "\n",
    "missing_counts_after = ussc_df.isna().sum()\n",
    "print(\"\\nMissing Values After Handling:\")\n",
    "print(missing_counts_after[missing_counts_after > 0])\n",
    "\n",
    "# Step 4: Save the Dataset After Handling Missing Values\n",
    "output_file = INTERMEDIATE_DIR / \"cleaned_data_after_subphase_3_2.csv\"\n",
    "ussc_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nDataset after handling missing values saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial checks on AGE:\n",
      "Missing in AGE: 0\n",
      "AGE describe:\n",
      " count    76314.000000\n",
      "mean        36.394633\n",
      "std         10.935215\n",
      "min         16.000000\n",
      "25%         28.000000\n",
      "50%         35.000000\n",
      "75%         43.000000\n",
      "max         86.000000\n",
      "Name: AGE, dtype: float64\n",
      "Created 'AGE_BIN' feature. Unique bins:\n",
      " AGE_BIN\n",
      "25-35    27526\n",
      "35-45    22707\n",
      "45+      16320\n",
      "<25       9761\n",
      "Name: count, dtype: int64\n",
      "Standardized numerical features: ['AGE', 'NUMDEPEN', 'CRIMHIST', 'SENTYR', 'SENTTOT']\n",
      "Created 'RACE_CITIZEN' feature.\n",
      "\n",
      "Describe DISPOSIT\n",
      "DISPOSIT\n",
      "Guilty plea                        74416\n",
      "Jury trial                          1742\n",
      "Nolo contendere                       70\n",
      "Trial by judge or bench trial         63\n",
      "Guilty plea and trial (>1count)       23\n",
      "Name: count, dtype: int64\n",
      "Wrote: /Users/araj/Documents/Code/Machine Learning/Bias Detection in Judicial Text /data/cleaned_data_phase3_unencoded_DISPOSIT.csv | shape=(76314, 19)\n",
      "\n",
      "Applied one-hot encoding to categorical features.\n",
      "Encoded features shape: (76314, 433)\n",
      "Wrote: /Users/araj/Documents/Code/Machine Learning/Bias Detection in Judicial Text /data/cleaned_data_phase3.csv | shape=(76314, 433)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load and Inspect the Dataset\n",
    "# --------------------------------------------------\n",
    "file_path = INTERMEDIATE_DIR / \"cleaned_data_after_subphase_3_2.csv\"\n",
    "ussc_df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "print(\"Initial checks on AGE:\")\n",
    "print(\"Missing in AGE:\", ussc_df[\"AGE\"].isna().sum())\n",
    "print(\"AGE describe:\\n\", ussc_df[\"AGE\"].describe())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Bin AGE Before Scaling\n",
    "# --------------------------------------------------\n",
    "age_bins = [0, 25, 35, 45, np.inf]\n",
    "age_labels = [\"<25\", \"25-35\", \"35-45\", \"45+\"]\n",
    "\n",
    "ussc_df[\"AGE_BIN\"] = pd.cut(\n",
    "    ussc_df[\"AGE\"],\n",
    "    bins=age_bins,\n",
    "    labels=age_labels,\n",
    "    right=False,\n",
    "    include_lowest=True,\n",
    ")\n",
    "print(\"Created 'AGE_BIN' feature. Unique bins:\\n\", ussc_df[\"AGE_BIN\"].value_counts(dropna=False))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Preserve raw targets before scaling\n",
    "# --------------------------------------------------\n",
    "# Keep a raw, interpretable copy for later modeling\n",
    "if \"SENTTOT\" in ussc_df.columns:\n",
    "    ussc_df[\"SENTTOT_RAW\"] = ussc_df[\"SENTTOT\"]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Standardize numeric features (keep raw copies above)\n",
    "# --------------------------------------------------\n",
    "numerical_features = [c for c in [\"AGE\", \"NUMDEPEN\", \"CRIMHIST\", \"SENTYR\", \"SENTTOT\"] if c in ussc_df.columns]\n",
    "scaler = StandardScaler()\n",
    "ussc_df[numerical_features] = scaler.fit_transform(ussc_df[numerical_features])\n",
    "print(\"Standardized numerical features:\", numerical_features)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Create Interaction Features\n",
    "# --------------------------------------------------\n",
    "if \"NEWRACE\" in ussc_df.columns and \"CITIZEN\" in ussc_df.columns:\n",
    "    ussc_df[\"RACE_CITIZEN\"] = ussc_df[\"NEWRACE\"].astype(str) + \"_\" + ussc_df[\"CITIZEN\"].astype(str)\n",
    "    print(\"Created 'RACE_CITIZEN' feature.\")\n",
    "\n",
    "print(\"\\nDescribe DISPOSIT\")\n",
    "if \"DISPOSIT\" in ussc_df.columns:\n",
    "    print(ussc_df[\"DISPOSIT\"].value_counts(dropna=False))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Write phase3 dataset that retains targets\n",
    "# --------------------------------------------------\n",
    "phase3_unencoded_path = DATA_DIR / \"cleaned_data_phase3_unencoded_DISPOSIT.csv\"\n",
    "ussc_df.to_csv(phase3_unencoded_path, index=False)\n",
    "print(f\"Wrote: {phase3_unencoded_path} | shape={ussc_df.shape}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Build one-hot feature matrix for modeling (drop targets)\n",
    "# --------------------------------------------------\n",
    "feature_df = ussc_df.copy()\n",
    "\n",
    "# If present, we drop target columns for the feature matrix.\n",
    "for c in [\"DISPOSIT\", \"SENTTOT\", \"SENTTOT_RAW\"]:\n",
    "    if c in feature_df.columns:\n",
    "        feature_df.drop(columns=[c], inplace=True)\n",
    "\n",
    "categorical_features = [\n",
    "    c\n",
    "    for c in [\n",
    "        \"NEWRACE\",\n",
    "        \"MONSEX\",\n",
    "        \"EDUCATN\",\n",
    "        \"CITIZEN\",\n",
    "        \"CITWHERE\",\n",
    "        \"ZONE\",\n",
    "        \"DISTRICT\",\n",
    "        \"CIRCDIST\",\n",
    "        \"CRIMLIV\",\n",
    "        \"AGE_BIN\",\n",
    "        \"RACE_CITIZEN\",\n",
    "        \"SENTMON\",\n",
    "    ]\n",
    "    if c in feature_df.columns\n",
    "]\n",
    "\n",
    "encoded_df = pd.get_dummies(feature_df, columns=categorical_features, drop_first=True)\n",
    "print(\"\\nApplied one-hot encoding to categorical features.\")\n",
    "print(\"Encoded features shape:\", encoded_df.shape)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. Save the transformed dataset (required by Notebook 4)\n",
    "# --------------------------------------------------\n",
    "phase3_features_path = DATA_DIR / \"cleaned_data_phase3.csv\"\n",
    "encoded_df.to_csv(phase3_features_path, index=False)\n",
    "print(f\"Wrote: {phase3_features_path} | shape={encoded_df.shape}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
